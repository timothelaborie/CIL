{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd().replace(\"\\\\\", \"/\")\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation,UperNetForSemanticSegmentation\n",
    "from glob import glob\n",
    "from albumentations import (\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,\n",
    "    RandomRotate90,\n",
    "    ShiftScaleRotate,\n",
    "    RandomBrightnessContrast,\n",
    "    CLAHE,\n",
    "    HueSaturationValue,\n",
    "    GaussNoise,\n",
    "    GridDistortion,\n",
    "    Compose,\n",
    "    RandomCrop\n",
    ")\n",
    "import cv2\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "kaggle = True if cwd == \"/kaggle/working\" else False\n",
    "data_path = \"/kaggle/input/\" if kaggle else cwd + \"/../../data/\"\n",
    "\n",
    "#takes path of x and returns x and y as images\n",
    "def get_label(x_path):\n",
    "    x_path = x_path.replace(\"\\\\\", \"/\")\n",
    "    if x_path.__contains__(\"massachusetts\"):\n",
    "        y_path = x_path.replace(\"tiff/train/\", \"tiff/train_labels/\").replace(\".tiff\", \".tif\")\n",
    "\n",
    "    if x_path.__contains__(\"ethz\") or x_path.__contains__(\"googlemaps\"):\n",
    "        y_path = x_path.replace(\"images/\", \"groundtruth/\")\n",
    "\n",
    "    if x_path.__contains__(\"deepglobe\"):\n",
    "        y_path = x_path.replace(\"sat.jpg\", \"mask.png\")\n",
    "\n",
    "    return Image.open(x_path), Image.open(y_path)\n",
    "\n",
    "\n",
    "def save(model, optim, name):\n",
    "    path = (\"/kaggle/working/\" if kaggle else \"\") + name + \".pth\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optim.state_dict(),\n",
    "    }, path)\n",
    "\n",
    "def load(model, optim, name):\n",
    "    path = (\"/kaggle/working/\" if kaggle else \"\") + name + \".pth\"\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and setup the classifier head for binary classification\n",
    "# model_name = \"nvidia/segformer-b5-finetuned-ade-640-640\"\n",
    "# model_name = \"nvidia/segformer-b0-finetuned-cityscapes-768-768\"\n",
    "\n",
    "# model_name = \"openmmlab/upernet-convnext-tiny\"\n",
    "model_name = \"openmmlab/upernet-convnext-xlarge\"\n",
    "\n",
    "if model_name.__contains__(\"nvidia\"):\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(model_name)\n",
    "    model.decode_head.classifier = nn.Sequential(\n",
    "        nn.Upsample((400, 400), mode='bilinear', align_corners=False),\n",
    "        nn.Conv2d(768 if model_name.__contains__(\"b5\") else 256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "    )\n",
    "\n",
    "elif model_name.__contains__(\"openmmlab\"):\n",
    "    model = UperNetForSemanticSegmentation.from_pretrained(model_name)\n",
    "    model.auxiliary_head.classifier = nn.Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "    model.decode_head.classifier = nn.Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "# Instantiate the feature extractor\n",
    "feature_extractor:SegformerImageProcessor = SegformerImageProcessor.from_pretrained(model_name, size=400)\n",
    "\n",
    "# Do a forward pass with random data to initialize the model\n",
    "x = torch.randn(2, 3, 400, 400).cuda()\n",
    "y = model(x).logits\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "print(\"model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_geometric_transforms_official():\n",
    "    geometric_transforms = [\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        RandomRotate90(p=0.5),\n",
    "    ]\n",
    "    return Compose(geometric_transforms, additional_targets={'mask':'image'})\n",
    "\n",
    "\n",
    "def get_photometric_transforms():\n",
    "    photometric_transforms = [\n",
    "        RandomBrightnessContrast(p=0.5),\n",
    "        CLAHE(p=0.5),\n",
    "        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "        GaussNoise(p=0.5)\n",
    "    ]\n",
    "    return Compose(photometric_transforms)\n",
    "\n",
    "\n",
    "return_orig_images = False\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_files, geometric_transform=None, photometric_transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.geometric_transform = geometric_transform\n",
    "        self.photometric_transform = photometric_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_orig, y_orig = get_label(self.image_files[idx])\n",
    "\n",
    "        x_orig:Image = x_orig.convert(\"RGB\")\n",
    "        y_orig:Image = y_orig.convert(\"RGB\")\n",
    "\n",
    "        if x_orig.size[0] == 1024:\n",
    "            x_orig = x_orig.resize((400, 400))\n",
    "            y_orig = y_orig.resize((400, 400))\n",
    "        \n",
    "        x_orig_np = np.array(x_orig, dtype=np.uint8)\n",
    "        y_orig_np = np.array(y_orig, dtype=np.uint8)\n",
    "\n",
    "        # Apply geometric transforms\n",
    "        x_augmented, y_augmented = x_orig_np.copy(), y_orig_np.copy()\n",
    "        if self.geometric_transform:\n",
    "            augmented = self.geometric_transform(image=x_augmented.copy(), mask=y_augmented.copy())\n",
    "            x_augmented, y_augmented = augmented['image'], augmented['mask']\n",
    "\n",
    "        # Apply photometric transforms\n",
    "        if self.photometric_transform:\n",
    "            augmented = self.photometric_transform(image=x_augmented.copy())\n",
    "            x_augmented = augmented['image']\n",
    "\n",
    "        x = feature_extractor(images=x_augmented.astype(np.float32), return_tensors=\"pt\").pixel_values.squeeze(0).cuda()\n",
    "        y = torch.tensor((y_augmented.astype(np.float32)/255)[:, :, 0], dtype=torch.float32).unsqueeze(0).cuda()\n",
    "\n",
    "        if return_orig_images:\n",
    "\n",
    "            # ensure the orig images have the same size when the datasets are combined\n",
    "            if x_orig_np.shape[0] == 1500:\n",
    "                x_orig_np = np.array(Image.fromarray(x_orig_np).resize((400, 400)))\n",
    "                y_orig_np = np.array(Image.fromarray(y_orig_np).resize((400, 400)))\n",
    "\n",
    "            # Convert the images to float32\n",
    "            x_orig_np = x_orig_np.astype(np.float32) / 255\n",
    "            y_orig_np = y_orig_np.astype(np.float32) / 255\n",
    "            x_augmented = x_augmented.astype(np.float32) / 255\n",
    "            y_augmented = y_augmented.astype(np.float32) / 255\n",
    "\n",
    "\n",
    "\n",
    "            return x, y, self.image_files[idx], x_orig_np, y_orig_np, x_augmented, y_augmented\n",
    "        \n",
    "        else:\n",
    "            return x, y, self.image_files[idx], torch.zeros(1), torch.zeros(1), torch.zeros(1), torch.zeros(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file = 'mass_files_cache.pkl'\n",
    "if os.path.exists(cache_file):\n",
    "    # If it exists, load the data from it\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        mass_files = pickle.load(f)\n",
    "else:\n",
    "    # If it doesn't exist, compute the data\n",
    "    mass_files_temp = glob(data_path + \"massachusetts-roads-dataset/tiff/train/*.tiff\")\n",
    "    #ignore files where over 10% of the pixels are white\n",
    "    mass_files = []\n",
    "    for file in mass_files_temp:\n",
    "        img = Image.open(file)\n",
    "        img = np.array(img)\n",
    "        frac = np.sum(img == 255) / (img.shape[0] * img.shape[1] * img.shape[2])\n",
    "        # print(file + \": \" + str(frac))\n",
    "        if frac < 0.1:\n",
    "            mass_files.append(file)\n",
    "    # Save the computed data to the cache file\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(mass_files, f)\n",
    "\n",
    "print(len(mass_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlemaps_dataset = CustomDataset(glob(data_path + \"googlemaps-boston-losangeles-suburbs/images/*.png\"), get_geometric_transforms_official(), get_photometric_transforms())\n",
    "googlemaps_loader = DataLoader(googlemaps_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dataset_len = len(glob(data_path + \"ethz-cil-road-segmentation-2023/training/images/*.png\"))\n",
    "\n",
    "# Split the dataset\n",
    "val_size = int(main_dataset_len * 0.2)\n",
    "train_size = main_dataset_len - val_size\n",
    "torch.manual_seed(0)\n",
    "indices = torch.randperm(main_dataset_len).tolist()\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "# Apply transformations only on training set\n",
    "train_dataset = CustomDataset(np.array(glob(data_path + \"ethz-cil-road-segmentation-2023/training/images/*.png\"))[train_indices], None, None)\n",
    "train_dataset_augmented = CustomDataset(np.array(glob(data_path + \"ethz-cil-road-segmentation-2023/training/images/*.png\"))[train_indices], get_geometric_transforms_official(), get_photometric_transforms())\n",
    "val_dataset = CustomDataset(np.array(glob(data_path + \"ethz-cil-road-segmentation-2023/training/images/*.png\"))[val_indices], None, None)\n",
    "val_dataset_augmented = CustomDataset(np.array(glob(data_path + \"ethz-cil-road-segmentation-2023/training/images/*.png\"))[val_indices], get_geometric_transforms_official(), get_photometric_transforms())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "train_loader_augmented = DataLoader(train_dataset_augmented, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "val_loader_augmented = DataLoader(val_dataset_augmented, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(model, loader, rows=4):\n",
    "    global return_orig_images\n",
    "    return_orig_images = True\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        fig, ax = plt.subplots(rows, 5, figsize=(40, 40))\n",
    "        for i, (x, y, name, x_orig, y_orig, x_augmented, y_augmented) in enumerate(loader):\n",
    "            x = x[0]\n",
    "            y = y[0]\n",
    "            name = name[0]\n",
    "            x_orig = x_orig[0]\n",
    "            y_orig = y_orig[0]\n",
    "            x_augmented = x_augmented[0]\n",
    "            y_augmented = y_augmented[0]\n",
    "\n",
    "            pred = model(x.unsqueeze(0)).logits.squeeze(0)\n",
    "\n",
    "            pred = F.sigmoid(pred).permute(1, 2, 0).cpu().numpy()\n",
    "            y = y.permute(1, 2, 0).cpu().numpy()\n",
    "            x = x.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "            ax[i][0].imshow(x_orig)\n",
    "            ax[i][1].imshow(y_orig)\n",
    "            ax[i][2].imshow(x_augmented)\n",
    "            ax[i][3].imshow(y_augmented)\n",
    "            ax[i][4].imshow(pred, cmap='gray')\n",
    "            \n",
    "\n",
    "            if i == rows - 1:\n",
    "                break\n",
    "            \n",
    "    model.train()\n",
    "    return_orig_images = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sample(model, massachusetts_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sample(model, deepglobe_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sample(model, combined_loader,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sample(model, googlemaps_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sample(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_sample(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "\n",
    "def train(model, dataset, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    for x, y, _a, _b, _c, _d, _e in tqdm(dataset):\n",
    "        x, y = x.cuda(), y.cuda()  \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x).logits\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        if steps % 100 == 0:\n",
    "            print(\"Training Loss:\", total_loss / steps)\n",
    "            if use_wandb: wandb.log({\"Train Loss\": total_loss / steps})\n",
    "\n",
    "    print(\"Training Loss:\", total_loss / len(dataset))\n",
    "    if use_wandb: wandb.log({\"Train Loss\": total_loss / len(dataset)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataset):\n",
    "    model.eval()\n",
    "    y_preds = np.array([], dtype=np.float32)\n",
    "    y_gt = np.array([], dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        for x, y, _a, _b, _c, _d, _e in dataset:\n",
    "            x, y = x.cuda(), y.cuda()  \n",
    "            # print(x.shape , y.shape)\n",
    "            y_pred = model(x).logits\n",
    "            y_pred = torch.sigmoid(y_pred)\n",
    "\n",
    "            # apply pooling to reduce the prediction from 400x400 to 25x25\n",
    "            y_pred = F.avg_pool2d(y_pred, 16, stride=16)\n",
    "            # apply pooling to reduce the label from 400x400 to 25x25\n",
    "            y = F.avg_pool2d(y, 16, stride=16)\n",
    "\n",
    "            y_preds = np.concatenate((y_preds, y_pred.cpu().numpy().flatten()))\n",
    "            y_gt = np.concatenate((y_gt, y.cpu().numpy().flatten()))\n",
    "            \n",
    "            \n",
    "    y_preds = np.array(y_preds)\n",
    "    y_gt = np.array(y_gt)\n",
    "    for tresh in np.arange(0.15,0.40,0.05):        \n",
    "        score = f1_score(y_gt>0.25, y_preds > tresh)\n",
    "        print(\"Validation F1 Score for tresh\",tresh,\":\", score)\n",
    "        if use_wandb: wandb.log(\"Validation F1 Score for tresh \"+str(tresh) +\": \" + str(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "if use_wandb: wandb.init(project=\"CIL 2023\", entity=\"tlaborie\")\n",
    "if use_wandb: wandb.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_official_dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "combined_official_loader = DataLoader(combined_official_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "combined_official_google_dataset = ConcatDataset([train_dataset_augmented, val_dataset_augmented, googlemaps_dataset])\n",
    "combined_official_google_loader = DataLoader(combined_official_google_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# actual_train_loader = train_loader\n",
    "actual_train_loader = combined_official_google_loader\n",
    "# actual_train_loader = googlemaps_loader\n",
    "\n",
    "actual_val_loader = combined_official_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the classification head and the last block\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.decode_head.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.auxiliary_head.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.backbone.encoder.stages[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "for epoch in range(2):\n",
    "    train(model, actual_train_loader, optimizer)\n",
    "    validate(model, actual_val_loader)\n",
    "\n",
    "\n",
    "\n",
    "#train everything except for the start\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.backbone.embeddings.parameters():\n",
    "    param.requires_grad = False\n",
    "# for param in model.segformer.encoder.block[0].parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "# save(model, optimizer, \"upernet_googlemaps_pre_warmup\")\n",
    "\n",
    "# visualize_sample(model, googlemaps_loader)\n",
    "# visualize_sample(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do a warmup epoch for the new optimizer\n",
    "optimizer.param_groups[0]['lr'] = 1e-9\n",
    "for epoch in range(1):\n",
    "    train(model, actual_train_loader, optimizer)\n",
    "    validate(model, actual_val_loader)\n",
    "\n",
    "save(model, optimizer, \"upernet_googlemaps_post_warmup\")\n",
    "\n",
    "# visualize_sample(model, googlemaps_loader)\n",
    "# visualize_sample(model, val_loader)\n",
    "optimizer.param_groups[0]['lr'] = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load(model, optimizer, \"model_googlemaps_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(8):\n",
    "    train(model, actual_train_loader, optimizer)\n",
    "    validate(model, actual_val_loader)\n",
    "\n",
    "save(model, optimizer, \"upernet_train_e-4\")\n",
    "\n",
    "visualize_sample(model, actual_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr'] = 1e-5\n",
    "\n",
    "for epoch in range(3):\n",
    "    train(model, actual_train_loader, optimizer)\n",
    "    validate(model, actual_val_loader)\n",
    "\n",
    "# save(model, optimizer, \"upernet_train_e-5\")\n",
    "\n",
    "optimizer.param_groups[0]['lr'] = 1e-6\n",
    "\n",
    "for epoch in range(2):\n",
    "    train(model, actual_train_loader, optimizer)\n",
    "    validate(model, actual_val_loader)\n",
    "\n",
    "save(model, optimizer, \"upernet_train_e-6\")\n",
    "\n",
    "visualize_sample(model, actual_val_loader)\n",
    "\n",
    "if use_wandb: wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load(model)\n",
    "# validate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sample(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_path = data_path + \"ethz-cil-road-segmentation-2023/\" + \"test/images/\"    \n",
    "    model.eval()\n",
    "    files = os.listdir(test_path)\n",
    "    for file in tqdm(files):\n",
    "        # print(test_path)\n",
    "        # print(file)\n",
    "        x_orig:Image = Image.open(test_path + file).convert(\"RGB\")\n",
    "        x_orig = np.array(x_orig, dtype=np.float32)\n",
    "        x = feature_extractor(images=x_orig, return_tensors=\"pt\").pixel_values.squeeze(0).cuda()\n",
    "        pred = model(x.unsqueeze(0)).logits\n",
    "        #pred = torch.repeat_interleave(torch.repeat_interleave(pred, 2, dim=2), 2, dim=3)\n",
    "        pred = pred.squeeze(0)\n",
    "        pred = torch.sigmoid(pred).permute(1, 2, 0).cpu().numpy()\n",
    "        # print(pred.shape) # (400, 400, 1)\n",
    "        pred = pred.squeeze(-1)\n",
    "        # print(pred.shape) # (400, 400)\n",
    "        pred = Image.fromarray((pred*255).astype(np.uint8))\n",
    "\n",
    "        output_dir = \"/kaggle/working/pred/\"\n",
    "        #make the folder\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        pred.save(output_dir + file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip the folder\n",
    "import shutil\n",
    "shutil.make_archive(\"/kaggle/working/pred\", 'zip', output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
