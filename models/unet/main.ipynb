{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\ncwd = os.getcwd().replace(\"\\\\\", \"/\")\nprint(cwd)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:44:34.136340Z","iopub.execute_input":"2023-07-17T09:44:34.137316Z","iopub.status.idle":"2023-07-17T09:44:34.148627Z","shell.execute_reply.started":"2023-07-17T09:44:34.137274Z","shell.execute_reply":"2023-07-17T09:44:34.147401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom PIL import Image\nimport requests\nimport wandb\nfrom sklearn.metrics import f1_score\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\nfrom glob import glob\nfrom albumentations import (\n    HorizontalFlip,\n    VerticalFlip,\n    RandomRotate90,\n    ShiftScaleRotate,\n    RandomBrightnessContrast,\n    CLAHE,\n    HueSaturationValue,\n    GaussNoise,\n    GridDistortion,\n    Compose,\n    RandomCrop,\n    Resize\n)\nimport cv2\nfrom torch.utils.data import ConcatDataset, DataLoader\n\nBATCH_SIZE = 4\n\nkaggle = True if cwd == \"/kaggle/working\" else False\ndata_path = \"/kaggle/input/\" if kaggle else cwd + \"/../../data/\"\n\n# takes path of x and returns x and y as images\ndef get_label(x_path):\n    x_path = x_path.replace(\"\\\\\",\"/\")\n    if x_path.__contains__(\"massachusetts\"):\n        y_path = x_path.replace(\"tiff/train/\", \"tiff/train_labels/\").replace(\".tiff\", \".tif\")\n    \n    if x_path.__contains__(\"ethz\") or x_path.__contains__(\"googlemaps\"):\n        y_path = x_path.replace(\"images/\", \"groundtruth/\")\n    \n    if x_path.__contains__(\"deepglobe\"):\n        y_path = x_path.replace(\"sat.jpg\", \"mask.png\")\n    \n    return Image.open(x_path), Image.open(y_path)\n\ndef save(model,optim,name):\n    path = (\"/kaggle/working/\" if kaggle else \"\") + name + \".pth\"\n    torch.save({\n        'model_state_dict' : model.state_dict(),\n        'optimizer_state_dict' : optim.state_dict(),\n    },path)\n\ndef load(model,optim, name):\n    path = (\"/kaggle/working/\" if kaggle else \"\") + name + \".pth\"\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optim.load_state_dict(checkpoint['optimizer_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:44:34.876122Z","iopub.execute_input":"2023-07-17T09:44:34.878774Z","iopub.status.idle":"2023-07-17T09:44:44.502706Z","shell.execute_reply.started":"2023-07-17T09:44:34.878729Z","shell.execute_reply":"2023-07-17T09:44:44.501709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ENCODER = 'resnet50'\nWEIGHTS = 'imagenet'\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {DEVICE}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:44:44.504745Z","iopub.execute_input":"2023-07-17T09:44:44.505136Z","iopub.status.idle":"2023-07-17T09:44:44.537893Z","shell.execute_reply.started":"2023-07-17T09:44:44.505101Z","shell.execute_reply":"2023-07-17T09:44:44.534896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_geometric_transforms_mass():\n    geometric_transforms = [\n        HorizontalFlip(p=0.5),\n        VerticalFlip(p=0.5),\n        RandomRotate90(p=0.5),\n        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=15, p=0.9, border_mode=cv2.BORDER_REFLECT),\n        GridDistortion(p=0.5),\n        #Â RandomCrop(height=400, width=400, p=1),\n        Resize(height=416, width=416, p=1),\n    ]\n    return Compose(geometric_transforms, additional_targets={'mask':'image'})\n\ndef get_geometric_transforms_deepglobe():\n    geometric_transforms = [\n        HorizontalFlip(p=0.5),\n        VerticalFlip(p=0.5),\n        RandomRotate90(p=0.5),\n        # ShiftScaleRotate(shift_limit=0.0625, scale_limit=0, rotate_limit=0, p=0.9, \n        #                 border_mode=cv2.BORDER_REFLECT),\n        # GridDistortion(p=0.5),\n        # RandomCrop(height=400, width=400, p=1),\n        Resize(height=416, width=416, p=1),\n    ]\n    return Compose(geometric_transforms, additional_targets={'mask':'image'})\ndef get_geometric_transforms_official():\n    geometric_transforms = [\n        HorizontalFlip(p=0.5),\n        VerticalFlip(p=0.5),\n        RandomRotate90(p=0.5),\n        Resize(height=416, width=416, p=1),\n    ]\n    return Compose(geometric_transforms, additional_targets={'mask':'image'})\n\n# Do not use\ndef get_photometric_transforms():\n    return None\n    photometric_transforms = [\n        RandomBrightnessContrast(p=0.5),\n        CLAHE(p=0.5),\n        HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n        GaussNoise(p=0.5)\n    ]\n    return Compose(photometric_transforms)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:44:44.541283Z","iopub.execute_input":"2023-07-17T09:44:44.541983Z","iopub.status.idle":"2023-07-17T09:44:44.554660Z","shell.execute_reply.started":"2023-07-17T09:44:44.541949Z","shell.execute_reply":"2023-07-17T09:44:44.553761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_extractor:SegformerImageProcessor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b5-finetuned-ade-640-640\", size=416)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:44:44.557876Z","iopub.execute_input":"2023-07-17T09:44:44.558766Z","iopub.status.idle":"2023-07-17T09:44:44.717336Z","shell.execute_reply.started":"2023-07-17T09:44:44.558732Z","shell.execute_reply":"2023-07-17T09:44:44.715319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, image_files, geometric_transform=None, photometric_transform=None):\n        self.image_files = image_files\n        self.geometric_transform = geometric_transform\n        self.photometric_transform = photometric_transform\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        x_orig, y_orig = get_label(self.image_files[idx])\n\n        x_orig:Image = x_orig.convert(\"RGB\")\n        y_orig:Image = y_orig.convert(\"RGB\")\n\n        if x_orig.size[0] != 416 or y_orig.size[0] != 416:\n            x_orig = x_orig.resize((416, 416))\n            y_orig = y_orig.resize((416, 416))\n        \n        x_orig_np = np.array(x_orig, dtype=np.uint8)\n        y_orig_np = np.array(y_orig, dtype=np.uint8)\n\n        # Apply geometric transforms\n        x_augmented, y_augmented = x_orig_np.copy(), y_orig_np.copy()\n        if self.geometric_transform:\n            augmented = self.geometric_transform(image=x_augmented.copy(), mask=y_augmented.copy())\n            x_augmented, y_augmented = augmented['image'], augmented['mask']\n\n        # Apply photometric transforms\n        # if self.photometric_transform:\n        #    augmented = self.photometric_transform(image=x_augmented.copy())\n        #    x_augmented = augmented['image']\n\n        x = feature_extractor(images=x_augmented.astype(np.float32), return_tensors=\"pt\").pixel_values.squeeze(0).cuda()\n        y = torch.tensor((y_augmented.astype(np.float32)/255)[:, :, 0], dtype=torch.float32)\n\n        # Convert the images to float32\n        x_orig_np = x_orig_np.astype(np.float32) / 255\n        y_orig_np = y_orig_np.astype(np.float32) / 255\n        x_augmented = x_augmented.astype(np.float32) / 255\n        y_augmented = y_augmented.astype(np.float32) / 255\n\n        return x, y, self.image_files[idx], x_orig_np, y_orig_np, x_augmented, y_augmented","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:44:44.718878Z","iopub.execute_input":"2023-07-17T09:44:44.719253Z","iopub.status.idle":"2023-07-17T09:44:44.732101Z","shell.execute_reply.started":"2023-07-17T09:44:44.719218Z","shell.execute_reply":"2023-07-17T09:44:44.730403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mass_files_temp = glob(data_path + \"massachusetts-roads-dataset/tiff/train/*.tiff\")\n#ignore files where over 10% of the pixels are white\nmass_files = []\nfor file in mass_files_temp:\n    img = Image.open(file)\n    img = np.array(img)\n    frac = np.sum(img == 255) / (img.shape[0] * img.shape[1] * img.shape[2])\n    # print(file + \": \" + str(frac))\n    if frac < 0.1:\n        mass_files.append(file)\n\nprint(len(mass_files))","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:44:44.734224Z","iopub.execute_input":"2023-07-17T09:44:44.734592Z","iopub.status.idle":"2023-07-17T09:46:48.327886Z","shell.execute_reply.started":"2023-07-17T09:44:44.734559Z","shell.execute_reply":"2023-07-17T09:46:48.326839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"massachusetts_dataset = CustomDataset(mass_files, get_geometric_transforms_mass(), get_photometric_transforms())\nmassachusetts_loader = DataLoader(massachusetts_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ndeepglobe_dataset = CustomDataset(glob(data_path + \"deepglobe-road-extraction-dataset/train/*.jpg\"), get_geometric_transforms_deepglobe(), get_photometric_transforms())\ndeepglobe_loader = DataLoader(deepglobe_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Combine both dataset mass and deepglobe\n\ncombined_dataset = ConcatDataset([massachusetts_dataset, deepglobe_dataset])\ncombined_loader = DataLoader(combined_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# Google Maps Dataset\n\ngooglemaps_dataset = CustomDataset(glob(data_path + \"googlemaps-boston-losangeles-suburbs/images/*.png\"), get_geometric_transforms_official())\ngooglemaps_loader = DataLoader(googlemaps_dataset, batch_size=BATCH_SIZE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:46:48.329267Z","iopub.execute_input":"2023-07-17T09:46:48.329996Z","iopub.status.idle":"2023-07-17T09:46:48.377092Z","shell.execute_reply.started":"2023-07-17T09:46:48.329937Z","shell.execute_reply":"2023-07-17T09:46:48.376106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_dataset_len = len(glob(data_path + \"ethz-cil-road-segmentation-2023/training/images/*.png\"))\n\nval_size = int(main_dataset_len * 0.2)\ntrain_size = main_dataset_len - val_size\ntorch.manual_seed(0)\nindices = torch.randperm(main_dataset_len).tolist()\ntrain_indices = indices[:train_size]\nval_indices = indices[train_size:]\n\n# Apply transformations only on training set\ntrain_dataset = CustomDataset(np.array(glob(data_path + \"ethz-cil-road-segmentation-2023/training/images/*.png\"))[train_indices], None, None)\ntrain_dataset_augmented = CustomDataset(np.array(glob(data_path + \"ethz-cil-road-segmentation-2023/training/images/*.png\"))[train_indices], get_geometric_transforms_official(), None)\nval_dataset = CustomDataset(np.array(glob(data_path + \"ethz-cil-road-segmentation-2023/training/images/*.png\"))[val_indices], None, None)\nval_dataset_augmented = CustomDataset(np.array(glob(data_path + \"ethz-cil-road-segmentation-2023/training/images/*.png\"))[val_indices], get_geometric_transforms_official(), None)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\ntrain_loader_augmented = DataLoader(train_dataset_augmented, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\nval_loader_augmented = DataLoader(val_dataset_augmented, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:46:48.378488Z","iopub.execute_input":"2023-07-17T09:46:48.378839Z","iopub.status.idle":"2023-07-17T09:46:48.398298Z","shell.execute_reply.started":"2023-07-17T09:46:48.378804Z","shell.execute_reply":"2023-07-17T09:46:48.397043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation_models_pytorch\nimport segmentation_models_pytorch as smp\nfrom segmentation_models_pytorch.losses import DiceLoss\n\nfrom torch import nn\n\nclass SegmentationModel(nn.Module):\n  def __init__(self):\n    super(SegmentationModel,self).__init__()\n\n    self.backbone = smp.Unet(\n        encoder_name = ENCODER,\n        encoder_weights = WEIGHTS,\n        in_channels = 3,\n        classes = 1,\n        activation = None\n    )\n\n  def forward(self, images, masks = None):\n    return self.backbone(images)\n\n    \nmodel = SegmentationModel()","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:46:48.399637Z","iopub.execute_input":"2023-07-17T09:46:48.400204Z","iopub.status.idle":"2023-07-17T09:47:03.098677Z","shell.execute_reply.started":"2023-07-17T09:46:48.400169Z","shell.execute_reply":"2023-07-17T09:47:03.097371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(DEVICE)\nprint(\"model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:47:03.104044Z","iopub.execute_input":"2023-07-17T09:47:03.104451Z","iopub.status.idle":"2023-07-17T09:47:05.784429Z","shell.execute_reply.started":"2023-07-17T09:47:03.104412Z","shell.execute_reply":"2023-07-17T09:47:05.783235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize_sample(model, loader):\n    with torch.no_grad():\n        rows = 4\n        fig, ax = plt.subplots(rows, 5, figsize=(40, 40))\n        for i, (x, y, name, x_orig, y_orig, x_augmented, y_augmented) in enumerate(loader):\n            x = x[0]\n            y = y[0]\n            name = name[0]\n            x_orig = x_orig[0]\n            y_orig = y_orig[0]\n            x_augmented = x_augmented[0]\n            y_augmented = y_augmented[0]\n\n            pred = model(x.unsqueeze(0)).squeeze(0)\n\n            pred = F.sigmoid(pred).permute(1, 2, 0).cpu().numpy()\n            if len(y.shape) == 2:\n                y = y.unsqueeze(0)\n            y = y.permute(1, 2, 0).cpu().numpy()\n            x = x.permute(1, 2, 0).cpu().numpy()\n\n            ax[i][0].imshow(x_orig)\n            ax[i][1].imshow(y_orig)\n            ax[i][2].imshow(x_augmented)\n            ax[i][3].imshow(y_augmented)\n            ax[i][4].imshow(pred, cmap='gray')\n            \n\n            if i == rows - 1:\n                break","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:47:05.786051Z","iopub.execute_input":"2023-07-17T09:47:05.786449Z","iopub.status.idle":"2023-07-17T09:47:05.798795Z","shell.execute_reply.started":"2023-07-17T09:47:05.786414Z","shell.execute_reply":"2023-07-17T09:47:05.797738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up training\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\ncriterion = nn.BCEWithLogitsLoss()","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:47:32.845150Z","iopub.execute_input":"2023-07-17T09:47:32.846295Z","iopub.status.idle":"2023-07-17T09:47:32.864025Z","shell.execute_reply.started":"2023-07-17T09:47:32.846221Z","shell.execute_reply":"2023-07-17T09:47:32.863021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, dataset, optimizer):\n    model.to(DEVICE)\n    model.train()\n    total_loss = 0\n    steps = 0\n    for x, y, _a, _b, _c, _d, _e in tqdm(dataset):\n        x, y = x.cuda(), y.unsqueeze(1).cuda()     \n        optimizer.zero_grad()\n        y_pred = model(x)\n        # y_pred = torch.repeat_interleave(torch.repeat_interleave(y_pred, 2, dim=2), 2, dim=3)\n        # y_pred = torch.repeat_interleave(torch.repeat_interleave(y_pred, 2, dim=2), 2, dim=3)\n        loss = criterion(y_pred, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        steps += 1\n        if steps % 100 == 0:\n            print(\"Training Loss:\", total_loss / steps)\n            # if use_wandb: wandb.log({\"Train Loss\": total_loss / steps})\n\n    print(\"Training Loss:\", total_loss / len(dataset))\n    # if use_wandb: wandb.log({\"Train Loss\": total_loss / len(dataset)})","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:47:32.865602Z","iopub.execute_input":"2023-07-17T09:47:32.866687Z","iopub.status.idle":"2023-07-17T09:47:32.875452Z","shell.execute_reply.started":"2023-07-17T09:47:32.866653Z","shell.execute_reply":"2023-07-17T09:47:32.874658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, dataset):\n    model.eval()\n    y_preds = np.array([], dtype=np.float32)\n    y_gt = np.array([], dtype=np.float32)\n    with torch.no_grad():\n        for x, y, _a, _b, _c, _d, _e in dataset:\n            x = x.cuda()\n            y = y.unsqueeze(1).cuda()  # add extra dimension to match model's output\n            y = F.interpolate(y, size=(416, 416), mode='bilinear', align_corners=False)\n            y_pred = model(x)\n            y_pred = torch.sigmoid(y_pred)\n            \n            # apply pooling to reduce the prediction from 400x400 to 25x25\n            y_pred = F.avg_pool2d(y_pred, 16, stride=16)\n            # apply pooling to reduce the label from 400x400 to 25x25\n            y = F.avg_pool2d(y, 16, stride=16)\n            \n            y_preds = np.concatenate((y_preds, y_pred.cpu().numpy().flatten()))\n            y_gt = np.concatenate((y_gt, y.cpu().numpy().flatten()))\n            \n    y_preds = np.array(y_preds)\n    y_gt = np.array(y_gt)\n    for tresh in np.arange(0.15,0.40,0.05):        \n        score = f1_score(y_gt>0.25, y_preds > tresh)\n        print(\"Validation F1 Score for tresh\",tresh,\":\", score)\n        # if use_wandb: wandb.log(\"Validation F1 Score for tresh \"+str(tresh) +\": \" + str(score))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:47:32.876795Z","iopub.execute_input":"2023-07-17T09:47:32.877363Z","iopub.status.idle":"2023-07-17T09:47:32.889662Z","shell.execute_reply.started":"2023-07-17T09:47:32.877332Z","shell.execute_reply":"2023-07-17T09:47:32.888882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_official_dataset = ConcatDataset([train_dataset, val_dataset])\ncombined_official_loader = DataLoader(combined_official_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\ncombined_official_google_dataset = ConcatDataset([train_dataset_augmented, googlemaps_dataset])\ncombined_official_google_loader = DataLoader(combined_official_google_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\nactual_pretrain_loader = googlemaps_loader\nactual_train_loader = combined_official_google_loader\nactual_val_loader = val_loader","metadata":{"execution":{"iopub.status.busy":"2023-07-17T10:21:42.449090Z","iopub.execute_input":"2023-07-17T10:21:42.450134Z","iopub.status.idle":"2023-07-17T10:21:42.457032Z","shell.execute_reply.started":"2023-07-17T10:21:42.450091Z","shell.execute_reply":"2023-07-17T10:21:42.455839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See how the modules are structured\n# for name, module in model.backbone.named_modules():\n#    print(name, module)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-07-17T09:50:43.624237Z","iopub.execute_input":"2023-07-17T09:50:43.625203Z","iopub.status.idle":"2023-07-17T09:50:43.647568Z","shell.execute_reply.started":"2023-07-17T09:50:43.625165Z","shell.execute_reply":"2023-07-17T09:50:43.645741Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Freeze all parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the last encoder layer, decoder and classification head\nfor param in model.backbone.encoder.layer4.parameters():\n    param.requires_grad = True\nfor param in model.backbone.decoder.parameters():\n    param.requires_grad = True\nfor param in model.backbone.segmentation_head.parameters():\n    param.requires_grad = True\n\nprint(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\noptimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n\nfor epoch in range(2):\n    train(model, actual_pretrain_loader, optimizer)\n    validate(model, actual_val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T10:00:55.503478Z","iopub.execute_input":"2023-07-17T10:00:55.503871Z","iopub.status.idle":"2023-07-17T10:04:32.198467Z","shell.execute_reply.started":"2023-07-17T10:00:55.503840Z","shell.execute_reply":"2023-07-17T10:04:32.197282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"module_encoder_first = nn.ModuleList([model.backbone.encoder.conv1, model.backbone.encoder.bn1, model.backbone.encoder.relu, model.backbone.encoder.maxpool])","metadata":{"execution":{"iopub.status.busy":"2023-07-17T10:07:03.881139Z","iopub.execute_input":"2023-07-17T10:07:03.881536Z","iopub.status.idle":"2023-07-17T10:07:03.888156Z","shell.execute_reply.started":"2023-07-17T10:07:03.881504Z","shell.execute_reply":"2023-07-17T10:07:03.886887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train everything except for the start of the encoder\nfor param in model.parameters():\n    param.requires_grad = True\nfor param in module_encoder_first.parameters():\n    param.requires_grad = False\n\nprint(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n\noptimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T10:07:58.193677Z","iopub.execute_input":"2023-07-17T10:07:58.194201Z","iopub.status.idle":"2023-07-17T10:07:58.207415Z","shell.execute_reply.started":"2023-07-17T10:07:58.194160Z","shell.execute_reply":"2023-07-17T10:07:58.206158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#do a warmup epoch for the new optimizer\noptimizer.param_groups[0]['lr'] = 1e-9\nfor epoch in range(1):\n    train(model, actual_pretrain_loader, optimizer)\n    validate(model, actual_val_loader)\n\nsave(model, optimizer, \"unet_post_warmup\")\n\noptimizer.param_groups[0]['lr'] = 1e-4","metadata":{"execution":{"iopub.status.busy":"2023-07-17T10:12:54.055716Z","iopub.execute_input":"2023-07-17T10:12:54.056336Z","iopub.status.idle":"2023-07-17T10:15:16.111558Z","shell.execute_reply.started":"2023-07-17T10:12:54.056294Z","shell.execute_reply":"2023-07-17T10:15:16.110126Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(6):\n    train(model, actual_train_loader, optimizer)\n    validate(model, actual_val_loader)\n    save(model, optimizer, \"unet_e-4\")","metadata":{"execution":{"iopub.status.busy":"2023-07-17T10:21:58.292962Z","iopub.execute_input":"2023-07-17T10:21:58.293968Z","iopub.status.idle":"2023-07-17T10:29:02.194685Z","shell.execute_reply.started":"2023-07-17T10:21:58.293932Z","shell.execute_reply":"2023-07-17T10:29:02.193193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer.param_groups[0]['lr'] = 1e-5\n\nfor epoch in range(2):\n    train(model, actual_train_loader, optimizer)\n    validate(model, actual_val_loader)\n\nsave(model, optimizer, \"unet_e-5\")","metadata":{"execution":{"iopub.status.busy":"2023-07-17T10:29:02.196637Z","iopub.execute_input":"2023-07-17T10:29:02.197086Z","iopub.status.idle":"2023-07-17T10:31:18.917695Z","shell.execute_reply.started":"2023-07-17T10:29:02.197038Z","shell.execute_reply":"2023-07-17T10:31:18.916594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(5):\n    train(model, combined_official_loader, optimizer)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T10:31:18.921105Z","iopub.execute_input":"2023-07-17T10:31:18.921769Z","iopub.status.idle":"2023-07-17T10:32:16.100043Z","shell.execute_reply.started":"2023-07-17T10:31:18.921729Z","shell.execute_reply":"2023-07-17T10:32:16.099098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_sample(model, actual_val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T10:33:19.620192Z","iopub.execute_input":"2023-07-17T10:33:19.621268Z","iopub.status.idle":"2023-07-17T10:33:27.338625Z","shell.execute_reply.started":"2023-07-17T10:33:19.621220Z","shell.execute_reply":"2023-07-17T10:33:27.337183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    test_path = data_path + \"ethz-cil-road-segmentation-2023/\" + \"test/images/\"    \n\n    files = os.listdir(test_path)\n    for file in tqdm(files):\n        # print(test_path)\n        # print(file)\n        x_orig:Image = Image.open(test_path + file).convert(\"RGB\")\n        x_orig = np.array(x_orig, dtype=np.float32)\n        x = feature_extractor(images=x_orig, return_tensors=\"pt\").pixel_values.squeeze(0).cuda()\n        pred = model(x.unsqueeze(0))\n        #pred = torch.repeat_interleave(torch.repeat_interleave(pred, 2, dim=2), 2, dim=3)\n        pred = pred.squeeze(0)\n        pred = torch.sigmoid(pred).permute(1, 2, 0).cpu().numpy()\n        # print(pred.shape) # (400, 400, 1)\n        pred = pred.squeeze(-1)\n        # pred = np.resize(pred, (400, 400))\n        # print(pred.shape) # (400, 400)\n        pred = Image.fromarray((pred*255).astype(np.uint8))\n        pred = pred.resize((400, 400))\n        print(pred.size)\n       \n\n        output_dir = \"/kaggle/working/pred/\"\n        #make the folder\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        \n        pred.save(output_dir + file)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T10:33:27.341177Z","iopub.execute_input":"2023-07-17T10:33:27.341728Z","iopub.status.idle":"2023-07-17T10:33:41.563463Z","shell.execute_reply.started":"2023-07-17T10:33:27.341692Z","shell.execute_reply":"2023-07-17T10:33:41.562483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# zip the folder\nimport shutil\nshutil.make_archive(\"/kaggle/working/pred\", 'zip', output_dir)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T10:33:44.513461Z","iopub.execute_input":"2023-07-17T10:33:44.513913Z","iopub.status.idle":"2023-07-17T10:33:44.807489Z","shell.execute_reply.started":"2023-07-17T10:33:44.513875Z","shell.execute_reply":"2023-07-17T10:33:44.806378Z"},"trusted":true},"execution_count":null,"outputs":[]}]}